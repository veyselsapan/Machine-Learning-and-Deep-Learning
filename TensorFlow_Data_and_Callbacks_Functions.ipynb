{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM6zHg7ueXpU/Abask1/ilT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["##**Data Augmentations**"],"metadata":{"id":"yh7vzYP_00Hq"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"9U9hxyJTz5rW"},"outputs":[],"source":["for i, (image, label) in enumerate(train_dataset.take(16)):\n","  ax = plt.subplot(4, 4, i + 1)\n","\n","  plt.imshow(image)\n","  plt.title(dataset_info.features['label'].int2str(label))\n","  plt.axis('off')"]},{"cell_type":"code","source":["def visualize(original, augmented):\n","  plt.subplot(1,2,1)\n","  plt.imshow(original)\n","\n","  plt.subplot(1,2,2)\n","  plt.imshow(augmented)"],"metadata":{"id":"TBfRJRj20PBl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["augmented_image = tf.image.adjust_saturation(original_image, saturation_factor = 0.3)\n","visualize(original_image, augmented_image)"],"metadata":{"id":"2a02GDW50VuW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["IM_SIZE = 224\n","original_image, label = next(iter(train_dataset))\n","@tf.function\n","def resize_rescale(image, label):\n","  #print(\"I was here\")\n","  #tf.print(\"I was here\")\n","  return tf.image.resize(image, (IM_SIZE, IM_SIZE))/255.0, label"],"metadata":{"id":"EbOz-HdH0bjq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### tf.keras.layer resizing and rescaling\n","resize_rescale_layers = tf.keras.Sequential([\n","       Resizing(IM_SIZE, IM_SIZE),\n","       Rescaling(1./255),\n","])"],"metadata":{"id":"JGhE5hHT0eR2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### tf.image augment\n","@tf.function\n","def augment(image, label):\n","  image, label = resize_rescale(image, label)\n","\n","  image = tf.image.rot90(image)\n","  #image = tf.image.adjust_saturation(image, saturation_factor = 0.3)\n","  image = tf.image.flip_left_right(image)\n","\n","  return image, label"],"metadata":{"id":"W-jZOcq-0hYP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class RotNinety(Layer):\n","  def __init__(self):\n","    super().__init__()\n","\n","  @tf.function\n","  def call(self, image):\n","    return tf.image.rot90(image)"],"metadata":{"id":"8BQC7itw0jWC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### tf.keras.layer augment\n","augment_layers = tf.keras.Sequential([\n","       RandomRotation(factor = (0.25, 0.2501),),\n","       RandomFlip(mode='horizontal',),\n","       RandomContrast(factor=0.1),\n","\n","])\n","\n","@tf.function\n","def augment_layer(image, label):\n","  return augment_layers(resize_rescale_layers(image), training = True), label"],"metadata":{"id":"kKUhAJVL0lfH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###**Mixup Data Augmentation**"],"metadata":{"id":"SgSYOMON06IL"}},{"cell_type":"code","source":["def mixup(train_dataset_1, train_dataset_2):\n","  (image_1,label_1), (image_2, label_2) = train_dataset_1, train_dataset_2\n","\n","  lamda = tfp.distributions.Beta(0.2,0.2)\n","  lamda = lamda.sample(1)[0]\n","\n","  image = lamda*image_1 + (1-lamda)*image_2\n","  label = lamda*tf.cast(label_1, dtype = tf.float32) + (1-lamda)*tf.cast(label_2, dtype = tf.float32)\n","  return image, label"],"metadata":{"id":"LOOm0L7u0n-7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###**CutMix Data Augmentation**"],"metadata":{"id":"Lp2IARIF1CP0"}},{"cell_type":"code","source":["def box(lamda):\n","\n","  r_x = tf.cast(tfp.distributions.Uniform(0, IM_SIZE).sample(1)[0], dtype = tf.int32)\n","  r_y = tf.cast(tfp.distributions.Uniform(0, IM_SIZE).sample(1)[0], dtype = tf.int32)\n","\n","  r_w = tf.cast(IM_SIZE*tf.math.sqrt(1-lamda), dtype = tf.int32)\n","  r_h = tf.cast(IM_SIZE*tf.math.sqrt(1-lamda), dtype = tf.int32)\n","\n","  r_x = tf.clip_by_value(r_x - r_w//2, 0, IM_SIZE)\n","  r_y = tf.clip_by_value(r_y - r_h//2, 0, IM_SIZE)\n","\n","  x_b_r = tf.clip_by_value(r_x + r_w//2, 0, IM_SIZE)\n","  y_b_r = tf.clip_by_value(r_y + r_h//2, 0, IM_SIZE)\n","\n","  r_w = x_b_r - r_x\n","  if(r_w == 0):\n","    r_w  = 1\n","\n","  r_h = y_b_r - r_y\n","  if(r_h == 0):\n","    r_h = 1\n","\n","  return r_y, r_x, r_h, r_w"],"metadata":{"id":"Pfxo1b3v1EHB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def cutmix(train_dataset_1, train_dataset_2):\n","  (image_1,label_1), (image_2, label_2) = train_dataset_1, train_dataset_2\n","\n","  lamda = tfp.distributions.Beta(0.2,0.2)\n","  lamda = lamda.sample(1)[0]\n","\n","  r_y, r_x, r_h, r_w = box(lamda)\n","  crop_2 = tf.image.crop_to_bounding_box(image_2, r_y, r_x, r_h, r_w)\n","  pad_2 = tf.image.pad_to_bounding_box(crop_2, r_y, r_x, IM_SIZE, IM_SIZE)\n","\n","  crop_1 = tf.image.crop_to_bounding_box(image_1, r_y, r_x, r_h, r_w)\n","  pad_1 = tf.image.pad_to_bounding_box(crop_1, r_y, r_x, IM_SIZE, IM_SIZE)\n","\n","  image = image_1 - pad_1 + pad_2\n","\n","  lamda = tf.cast(1- (r_w*r_h)/(IM_SIZE*IM_SIZE), dtype = tf.float32)\n","  label = lamda*tf.cast(label_1, dtype = tf.float32) + (1-lamda)*tf.cast(label_2, dtype = tf.float32)\n","\n","  return image, label"],"metadata":{"id":"zZ-LXg6X1HEJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###**Albumentations**"],"metadata":{"id":"7Z-yFSKU1OYB"}},{"cell_type":"code","source":["transforms = A.Compose(\n","    [\n","      A.Resize(IM_SIZE, IM_SIZE),\n","\n","      A.OneOf([A.HorizontalFlip(),\n","                A.VerticalFlip(),], p = 0.3),\n","\n","      A.RandomRotate90(),\n","      #A.RandomGridShuffle(grid=(3, 3), always_apply=False, p=0.5),\n","      A.RandomBrightnessContrast(brightness_limit=0.2,\n","                                contrast_limit=0.2,\n","                                always_apply=False, p=0.5),\n","      #A.Cutout(num_holes=8, max_h_size=8, max_w_size=8, fill_value=0, always_apply=False, p=0.5),\n","      A.Sharpen(alpha=(0.2, 0.5), lightness=(0.5, 1.0), always_apply=False, p=0.5),\n","])"],"metadata":{"id":"Mbjmro5g1JP-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def aug_albument(image):\n","  data = {\"image\":image}\n","  image = transforms(**data)\n","  image = image[\"image\"]\n","  image = tf.cast(image/255., tf.float32)\n","  return image"],"metadata":{"id":"vEqKdVm61SNp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def process_data(image, label):\n","    aug_img = tf.numpy_function(func=aug_albument, inp=[image], Tout=tf.float32)\n","    return aug_img, label"],"metadata":{"id":"KmSbXEPr1UHI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###**Repeating the dataset (x5)**"],"metadata":{"id":"_yDnVtpX1bUh"}},{"cell_type":"code","source":["def augment_1(image, label):\n","  image, label = resize_rescale(image, label)\n","\n","  image = tf.image.random_brightness(image, 0.2)\n","  return image, label"],"metadata":{"id":"h2nwSLG71dXp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def augment_2(image, label):\n","  image, label = resize_rescale(image, label)\n","\n","  image = tf.image.random_flip_up_down(image)\n","  return image, label"],"metadata":{"id":"l1RCQzVe1gmy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def augment_3(image, label):\n","  image, label = resize_rescale(image, label)\n","\n","  image = tf.image.flip_left_right(image)\n","  return image, label"],"metadata":{"id":"4nlyFx2T1ipF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def augment_4(image, label):\n","  image, label = resize_rescale(image, label)\n","\n","  image = tf.image.rot90(image)\n","  return image, label"],"metadata":{"id":"Kru9rs3L1lA4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def augment_5(image, label):\n","  image, label = resize_rescale(image, label)\n","\n","  return image, label"],"metadata":{"id":"zZqTg-uD1mz_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##**Callbacks**"],"metadata":{"id":"lutuOJTF17ZG"}},{"cell_type":"code","source":["class LossCallback(Callback):\n","  def on_epoch_end(self, epoch, logs):\n","    print(\"\\n For Epoch Number {} the model has a loss of {} \".format(epoch+1, logs[\"loss\"]))\n","\n","  def on_batch_end(self, batch, logs):\n","    print(\"\\n For Batch Number {} the model has a loss of {} \".format(batch+1, logs))"],"metadata":{"id":"S0HPo8Uw19SD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class LogImagesCallbackTensorBoard(Callback):\n","  def on_epoch_end(self, epoch, logs):\n","    labels = []\n","    inp = []\n","\n","    for x,y in test_dataset.as_numpy_iterator():\n","      labels.append(y)\n","      inp.append(x)\n","    labels = np.array([i[0] for i in labels])\n","    predicted = lenet_model.predict(np.array(inp)[:,0,...])\n","\n","    threshold = 0.5\n","\n","    cm = confusion_matrix(labels, predicted > threshold)\n","\n","    plt.figure(figsize=(8,8))\n","\n","    sns.heatmap(cm, annot=True,)\n","    plt.title('Confusion matrix - {}'.format(threshold))\n","    plt.ylabel('Actual')\n","    plt.xlabel('Predicted')\n","    plt.axis('off')\n","\n","    buffer = io.BytesIO()\n","    plt.savefig(buffer, format = 'png')\n","\n","    image = tf.image.decode_png(buffer.getvalue(), channels=3)\n","    image = tf.expand_dims(image, axis = 0)\n","\n","    CURRENT_TIME = datetime.datetime.now().strftime('%d%m%y - %h%m%s')\n","    IMAGE_DIR = './logs/' + CURRENT_TIME + '/images'\n","    image_writer = tf.summary.create_file_writer(IMAGE_DIR)\n","\n","    with image_writer.as_default():\n","      tf.summary.image(\"Training data\", image, step = epoch)"],"metadata":{"id":"T3aaCL0w1_Fj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class LogImagesCallbackWandBPlot(Callback):\n","  def on_epoch_end(self, epoch, logs):\n","    labels = []\n","    inp = []\n","\n","    for x,y in test_dataset.as_numpy_iterator():\n","      labels.append(y)\n","      inp.append(x)\n","    labels = np.array([i[0] for i in labels])\n","    predicted = lenet_model.predict(np.array(inp)[:,0,...])\n","\n","    print(\"labels\", labels, labels.dtype)\n","    print(\"predicted\", predicted, predicted.dtype)\n","\n","    pred = []\n","\n","    for i in range(len(predicted)):\n","      if(predicted[i][0] < 0.5):\n","        pred.append([1,0])\n","      else:\n","        pred.append([0,1])\n","\n","    pred = np.array(pred)\n","\n","    # wandb.log({\"Confusion Matrix\" : wandb.plot.confusion_matrix(\n","    #     probs = pred,\n","    #     y_true=labels,\n","    #     class_names=[\"Parasitized\", \"Uninfected\"])})\n","\n","    wandb.log({\"ROC Curve\" : wandb.plot.roc_curve(\n","        y_true = labels,\n","        y_probas = pred,\n","        labels = ['Parasitized', 'Uninfected'],\n","    )})\n","\n","    wandb.log({'loss':logs['loss']})"],"metadata":{"id":"nCUw32_E2Bao"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class LogImagesCallbackWandB(Callback):\n","  def on_epoch_end(self, epoch, logs):\n","    labels = []\n","    inp = []\n","\n","    for x,y in test_dataset.as_numpy_iterator():\n","      labels.append(y)\n","      inp.append(x)\n","    labels = np.array([i[0] for i in labels])\n","    predicted = lenet_model.predict(np.array(inp)[:,0,...])\n","\n","    threshold = 0.5\n","\n","    cm = confusion_matrix(labels, predicted > threshold)\n","\n","    plt.figure(figsize=(8,8))\n","\n","    sns.heatmap(cm, annot=True,)\n","    plt.title('Confusion matrix - {}'.format(threshold))\n","    plt.ylabel('Actual')\n","    plt.xlabel('Predicted')\n","    plt.axis('off')\n","\n","    buffer = io.BytesIO()\n","    plt.savefig(buffer, format = 'png')\n","\n","    image_array = tf.image.decode_png(buffer.getvalue(), channels=3)\n","\n","    images = wandb.Image(image_array, caption=\"Confusion Matrix for epoch: {}\".format(epoch))\n","\n","    wandb.log(\n","        {\"Confusion Matrix\": images})\n",""],"metadata":{"id":"uxDuUeNe2DZ3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###**CSV Logger**"],"metadata":{"id":"iZ3IYI3m2I8w"}},{"cell_type":"code","source":["csv_callback = CSVLogger(\n","    'logs.csv', separator=',', append=True\n",")"],"metadata":{"id":"QriOGsgp2Fen"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###**Early Stopping**"],"metadata":{"id":"aTe_aU0l2PHW"}},{"cell_type":"code","source":["es_callback = EarlyStopping(\n","    monitor='val_loss', min_delta=0, patience=2, verbose=1,\n","    mode='auto', baseline=None, restore_best_weights=False\n",")"],"metadata":{"id":"af1-SFSw2Mfs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###**Tensorboard**"],"metadata":{"id":"jZwfh1Th2Ubd"}},{"cell_type":"code","source":["pip install -U tensorboard_plugin_profile"],"metadata":{"id":"_wE35kcg2bVZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["CURRENT_TIME = datetime.datetime.now().strftime('%d%m%y - %h%m%s')\n","METRIC_DIR = './logs/' + CURRENT_TIME + '/metrics'\n","train_writer = tf.summary.create_file_writer(METRIC_DIR)"],"metadata":{"id":"B2EORxKx2anr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["LOG_DIR = './logs/'+ CURRENT_TIME\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=LOG_DIR, histogram_freq = 1, profile_batch = '100,132')"],"metadata":{"id":"2z7XCeWa2c-I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###**LearningRateScheduler**"],"metadata":{"id":"ApWk1Joj2hUW"}},{"cell_type":"code","source":["def scheduler(epoch, lr):\n","\n","  if epoch <= 1:\n","    learning_rate = lr\n","  else:\n","    learning_rate = lr * tf.math.exp(-0.1)\n","    learning_rate = learning_rate.numpy()\n","\n","  with train_writer.as_default():\n","    tf.summary.scalar('Learning Rate', data = learning_rate, step = epoch)\n","  return learning_rate\n","scheduler_callback = LearningRateScheduler(scheduler, verbose = 1)"],"metadata":{"id":"VIJGSvoM2jAV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###**Model Checkpointing**"],"metadata":{"id":"l8EVuQAF2pP6"}},{"cell_type":"code","source":["checkpoint_callback = ModelCheckpoint(\n","    'weights.{epoch:02d}-{val_loss:.2f}.hdf5', monitor='val_precision', verbose=0, save_best_only=True,\n","    save_weights_only=True, mode='auto', save_freq='epoch',\n",")"],"metadata":{"id":"A5jqo6NF2sOd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###**ReduceLearningRateOnPlateau**"],"metadata":{"id":"UZmjwmlO2uW-"}},{"cell_type":"code","source":["plateau_callback = ReduceLROnPlateau(\n","    monitor='val_accuracy', factor=0.1, patience=5, verbose=1\n",")"],"metadata":{"id":"sPmc8rjZ2vyU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##**Custom Metric Class**"],"metadata":{"id":"cMpv2iM520GQ"}},{"cell_type":"code","source":["class CustomAccuracy(tf.keras.metrics.Metric):\n","  def __init__(self, name = 'Custom_Accuracy', FACTOR = 1):\n","    super(CustomAccuracy, self).__init__()\n","    self.FACTOR = FACTOR\n","    self.accuracy = self.add_weight(name = name, initializer = 'zeros')\n","\n","\n","  def update_state(self, y_true, y_pred, sample_weight = None):\n","    output = binary_accuracy(tf.cast(y_true, dtype = tf.float32), y_pred)*self.FACTOR\n","    self.accuracy.assign(tf.math.count_nonzero(output, dtype = tf.float32)/tf.cast(len(output), dtype = tf.float32))\n","\n","  def result(self):\n","    return self.accuracy\n","\n","  def reset_states(self):\n","    self.accuracy.assign(0.)"],"metadata":{"id":"1TY66lRL21qu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###**Custom Metric Method (without parametres)**"],"metadata":{"id":"1q2_VMDu25rA"}},{"cell_type":"code","source":["def custom_accuracy(y_true, y_pred):\n","  print(binary_accuracy(y_true, y_pred))\n","  return binary_accuracy(y_true, y_pred)"],"metadata":{"id":"cDlyu_uw27UK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###**Custom Metric Method (with Parametres)**"],"metadata":{"id":"KWgrYlCS29YT"}},{"cell_type":"code","source":["def custom_accuracy(FACTOR):\n","  def metric(y_true, y_pred):\n","    return binary_accuracy(y_true, y_pred)* FACTOR\n","  return metric"],"metadata":{"id":"UlcrnzzJ3Aeh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###**Custom Loss Class**"],"metadata":{"id":"L_Oa54pY3CzU"}},{"cell_type":"code","source":["class CustomBCE(tf.keras.losses.Loss):\n","  def __init__(self, FACTOR):\n","    super(CustomBCE, self).__init__()\n","    self.FACTOR = FACTOR\n","  def call(self, y_true, y_pred):\n","    bce = BinaryCrossentropy()\n","    return bce(y_true, y_pred)* self.FACTOR"],"metadata":{"id":"ObALley_3GUR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###**Custom Loss Method (with parametres)**"],"metadata":{"id":"L5ZFWn5x3J-W"}},{"cell_type":"code","source":["def custom_bce(FACTOR):\n","  def loss(y_true, y_pred):\n","    bce = BinaryCrossentropy()\n","    return bce(y_true, y_pred)* FACTOR\n","  return loss"],"metadata":{"id":"XV8-Zvhn3LcS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###**Custom Loss Method (without parametres)**"],"metadata":{"id":"RRMukpVi3Niw"}},{"cell_type":"code","source":["def custom_bce(y_true, y_pred):\n","  bce = BinaryCrossentropy()\n","  return bce(y_true, y_pred)"],"metadata":{"id":"ZlfC5Ya43QyO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##**Hyperparameter Tuning**"],"metadata":{"id":"5gtYPkG13WvJ"}},{"cell_type":"code","source":["IM_SIZE = 224\n","def model_tune(hparams):\n","  lenet_model = tf.keras.Sequential([\n","    InputLayer(input_shape = (IM_SIZE, IM_SIZE, 3)),\n","\n","    Conv2D(filters = 6, kernel_size = 3, strides=1, padding='valid',\n","          activation = 'relu',kernel_regularizer = L2(hparams[HP_REGULARIZATION_RATE])),\n","    BatchNormalization(),\n","    MaxPool2D (pool_size = 2, strides= 2),\n","    Dropout(rate = hparams[HP_DROPOUT]),\n","\n","    Conv2D(filters = 16, kernel_size = 3, strides=1, padding='valid',\n","          activation = 'relu', kernel_regularizer = L2(hparams[HP_REGULARIZATION_RATE])),\n","    BatchNormalization(),\n","    MaxPool2D (pool_size = 2, strides= 2),\n","\n","    Flatten(),\n","\n","    Dense( hparams[HP_NUM_UNITS_1], activation = \"relu\", kernel_regularizer = L2(hparams[HP_REGULARIZATION_RATE])),\n","    BatchNormalization(),\n","    Dropout(rate = hparams[HP_DROPOUT]),\n","\n","    Dense(hparams[HP_NUM_UNITS_2], activation = \"relu\", kernel_regularizer = L2(hparams[HP_REGULARIZATION_RATE])),\n","    BatchNormalization(),\n","\n","    Dense(1, activation = \"sigmoid\"),\n","  ])\n","\n","  lenet_model.compile(\n","        optimizer= Adam(learning_rate = hparams[HP_LEARNING_RATE]),\n","        loss='binary_crossentropy',\n","        metrics=['accuracy'],\n","    )\n","\n","  lenet_model.fit(val_dataset, epochs=1)\n","  _, accuracy = lenet_model.evaluate(val_dataset)\n","  return accuracy"],"metadata":{"id":"PX31Vh0t3YIr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["HP_NUM_UNITS_1 = hp.HParam('num_units_1', hp.Discrete([16,32,64,128]))\n","HP_NUM_UNITS_2 = hp.HParam('num_units_2', hp.Discrete([16,32,64,128]))\n","HP_DROPOUT = hp.HParam('dropout_rate', hp.Discrete([0.1,0.2,0.3]))\n","HP_REGULARIZATION_RATE = hp.HParam('regularization_rate', hp.Discrete([0.001,0.01,0.1]))\n","HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([1e-4, 1e-3]))"],"metadata":{"id":"q40_4WQD3Z52"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["run_number = 0\n","for num_units_1 in HP_NUM_UNITS_1.domain.values:\n","  for num_units_2 in HP_NUM_UNITS_2.domain.values:\n","    for dropout_rate in HP_DROPOUT.domain.values:\n","      for regularization_rate in HP_REGULARIZATION_RATE.domain.values:\n","        for learning_rate in HP_LEARNING_RATE.domain.values:\n","\n","          hparams = {\n","              HP_NUM_UNITS_1: num_units_1,\n","              HP_NUM_UNITS_2: num_units_2,\n","              HP_DROPOUT: dropout_rate,\n","              HP_REGULARIZATION_RATE: regularization_rate,\n","              HP_LEARNING_RATE: learning_rate,\n","\n","          }\n","          file_writer = tf.summary.create_file_writer('logs/hparams-' + str(run_number))\n","\n","          with file_writer.as_default():\n","              hp.hparams(hparams)\n","              accuracy = model_tune(hparams)\n","              tf.summary.scalar('accuracy', accuracy, step = 0)\n","          print(\"For the run {}, hparams num_units_1:{}, num_units_2:{}, dropout:{}, regularization_rate:{}, learning_rate:{}\".format(run_number, hparams[HP_NUM_UNITS_1], hparams[HP_NUM_UNITS_2],\n","                                                             hparams[HP_DROPOUT], hparams[HP_REGULARIZATION_RATE],\n","                                                             hparams[HP_LEARNING_RATE]))\n","          run_number += 1"],"metadata":{"id":"GlJbVq703boz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##**Custom Training Loop**"],"metadata":{"id":"Lt7hvX--3fOG"}},{"cell_type":"code","source":["OPTIMIZER = Adam(learning_rate = 0.01)\n","METRIC = BinaryAccuracy()\n","METRIC_VAL = BinaryAccuracy()\n","EPOCHS = CONFIGURATION['N_EPOCHS']"],"metadata":{"id":"y29FjopV3iF4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["CURRENT_TIME = datetime.datetime.now().strftime('%d%m%y - %h%m%s')\n","CUSTOM_TRAIN_DIR = './logs/' + CURRENT_TIME + '/custom/train'\n","CUSTOM_VAL_DIR = './logs/' + CURRENT_TIME + '/custom/val'\n","\n","custom_train_writer = tf.summary.create_file_writer(CUSTOM_TRAIN_DIR)\n","custom_val_writer = tf.summary.create_file_writer(CUSTOM_VAL_DIR)"],"metadata":{"id":"-82NcHs33jts"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@tf.function\n","def training_block(x_batch, y_batch):\n","  with tf.GradientTape() as recorder:\n","      y_pred = lenet_model(x_batch, training = True)\n","      loss = custom_bce(y_batch, y_pred)\n","\n","  #wandb.log({'loss':loss.numpy()})\n","  partial_derivatives = recorder.gradient(loss, lenet_model.trainable_weights)\n","  OPTIMIZER.apply_gradients(zip(partial_derivatives, lenet_model.trainable_weights))\n","  METRIC.update_state(y_batch, y_pred)\n","  return loss\n","\n","@tf.function\n","def val_block(x_batch_val, y_batch_val):\n","    y_pred_val = lenet_model(x_batch_val, training = False)\n","    loss_val = custom_bce(y_batch_val, y_pred_val)\n","    METRIC_VAL.update_state(y_batch_val, y_pred_val)\n","    return loss_val"],"metadata":{"id":"eBueXLA83laC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def CustomTraining(model, loss_function, METRIC, VAL_METRIC, OPTIMIZER, train_dataset, val_dataset, EPOCHS):\n","  for epoch in range(EPOCHS):\n","    print(\"Training starts for epoch number {}\".format(epoch+1))\n","    for step, (x_batch, y_batch) in enumerate(train_dataset):\n","      loss = training_block(x_batch, y_batch)\n","\n","    print(\"Training Loss\", loss)\n","    print(\"The accuracy is: \", METRIC.result())\n","\n","    with custom_train_writer.as_default():\n","      tf.summary.scalar('Training Loss', data = loss, step = epoch)\n","    with custom_train_writer.as_default():\n","      tf.summary.scalar('Training Accuracy', data = METRIC.result(), step = epoch)\n","\n","    METRIC.reset_states()\n","\n","    for (x_batch_val, y_batch_val) in val_dataset:\n","      loss_val = val_block(x_batch_val, y_batch_val)\n","\n","    print(\"The Validation loss\", loss_val)\n","    print(\"The Validation accuracy is: \", METRIC_VAL.result())\n","\n","    with custom_val_writer.as_default():\n","      tf.summary.scalar('Validation Loss', data = loss_val, step = epoch)\n","    with custom_val_writer.as_default():\n","      tf.summary.scalar('Validation Accuracy', data = METRIC_VAL.result(), step = epoch)\n","\n","    METRIC_VAL.reset_states()\n","  print(\"Training Complete!!!!\")"],"metadata":{"id":"fjncdI-E3o3X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["CustomTraining(lenet_model, custom_bce, METRIC, METRIC_VAL, OPTIMIZER, train_dataset, val_dataset, EPOCHS)"],"metadata":{"id":"IhrQEcZE3u7h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##**Visualizations**"],"metadata":{"id":"8Q4p3EjJ3zV0"}},{"cell_type":"code","source":["%load_ext tensorboard"],"metadata":{"id":"Et_pxxWK31bL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tensorboard --logdir=logs"],"metadata":{"id":"sF1WiP1Z33LT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('Model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train_loss', 'val_loss'])\n","plt.show()"],"metadata":{"id":"HdOHyEnV34sA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.title('Model Accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['train_accuracy', 'val_accuracy'])\n","plt.show()"],"metadata":{"id":"y_mIDpnl37fK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###**Visualization Confusion Matrix**"],"metadata":{"id":"WtkJp_aZ4JxA"}},{"cell_type":"code","source":["labels = []\n","inp = []\n","# for t in test_dataset:\n","#   print(t)\n","#   break\n","for x,y in test_dataset.as_numpy_iterator():\n","  labels.append(y)\n","  inp.append(x)"],"metadata":{"id":"LdSDmDme4NnR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(np.array(inp).shape)\n","print(np.array(inp)[:,0,...].shape)"],"metadata":{"id":"Kj4923d34PVi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels = np.array([i[0] for i in labels])\n","print(labels)"],"metadata":{"id":"bpBHfjoY4P1z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predicted = lenet_model.predict(np.array(inp)[:,0,...])\n","print(predicted[:,0])"],"metadata":{"id":"2q5ceJqi4RYv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["threshold = 0.5\n","\n","cm = confusion_matrix(labels, predicted > threshold)\n","print(cm)\n","plt.figure(figsize=(8,8))\n","\n","sns.heatmap(cm, annot=True,)\n","plt.title('Confusion matrix - {}'.format(threshold))\n","plt.ylabel('Actual')\n","plt.xlabel('Predicted')\n"],"metadata":{"id":"nikXreNy4S8f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###**ROC Plots**"],"metadata":{"id":"Ja70FO_B4WyY"}},{"cell_type":"code","source":["fp, tp, thresholds = roc_curve(labels, predicted)\n","plt.plot(fp, tp)\n","plt.xlabel(\"False Positive rate\")\n","plt.ylabel(\"True Positive rate\")\n","\n","plt.grid()\n","\n","skip = 20\n","\n","for i in range(0, len(thresholds), skip):\n","  plt.text(fp[i], tp[i], thresholds[i])\n","\n","plt.show()"],"metadata":{"id":"lb6PR-ks4Yuz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["parasite_or_not(lenet_model.predict(test_dataset.take(1))[0][0])"],"metadata":{"id":"Ml_WKEcg4bAb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def parasite_or_not(x):\n","  if(x<0.5):\n","    return str('P')\n","  else:\n","    return str('U')"],"metadata":{"id":"uUJpE3Up4elm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i, (image, label) in enumerate(test_dataset.take(9)):\n","\n","  ax = plt.subplot(3, 3, i + 1)\n","  plt.imshow(image[0])\n","  plt.title(str(parasite_or_not(label.numpy()[0])) + \":\" + str(parasite_or_not(lenet_loaded_model.predict(image)[0][0])))\n","\n","  plt.axis('off')"],"metadata":{"id":"zF_48nmU4gXU"},"execution_count":null,"outputs":[]}]}