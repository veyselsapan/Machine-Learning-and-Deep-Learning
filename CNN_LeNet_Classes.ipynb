{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPGkPQbQpQEBKfHsG13JMU6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"lGyGyP5_4AgR"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import sklearn\n","import cv2\n","from sklearn.metrics import confusion_matrix, roc_curve\n","import seaborn as sns\n","import datetime\n","import io\n","import os\n","import random\n","from google.colab import files\n","from PIL import Image\n","import albumentations as A\n","import tensorflow_datasets as tfds\n","import tensorflow_probability as tfp\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Layer\n","from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Flatten, InputLayer, BatchNormalization, Input, Dropout, RandomFlip, RandomRotation, Resizing, Rescaling\n","from tensorflow.keras.losses import BinaryCrossentropy\n","from tensorflow.keras.metrics import BinaryAccuracy, FalsePositives, FalseNegatives, TruePositives, TrueNegatives, Precision, Recall, AUC, binary_accuracy\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import Callback, CSVLogger, EarlyStopping, LearningRateScheduler, ModelCheckpoint, ReduceLROnPlateau\n","from tensorflow.keras.regularizers  import L2, L1\n","from tensorboard.plugins.hparams import api as hp"]},{"cell_type":"code","source":["dataset, info = tfds.load('malaria', as_supervised=True, with_info=True, shuffle_files=True, split=['train'])"],"metadata":{"id":"QbYF3R7z6uJO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset"],"metadata":{"id":"XcRTTTYo6u-3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["info"],"metadata":{"id":"LQxIhOif6xqP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def splits(dataset, TRAIN_RATIO, VAL_RATIO, TEST_RATIO):\n","  DATASET_SIZE = len(dataset)\n","  TRAIN_SIZE = int(TRAIN_RATIO * DATASET_SIZE)\n","  VAL_SIZE = int(VAL_RATIO * DATASET_SIZE)\n","  TEST_SIZE = int(TEST_RATIO * DATASET_SIZE)\n","  return dataset.take(TRAIN_SIZE), dataset.skip(TRAIN_SIZE).take(VAL_SIZE), dataset.skip(TRAIN_SIZE).skip(VAL_SIZE).take(TEST_SIZE)"],"metadata":{"id":"wcaf7DbA6yo8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["TRAIN_RATIO = 0.8\n","VAL_RATIO = 0.1\n","TEST_RATIO = 0.1\n","train, val, test = splits(dataset[0], TRAIN_RATIO, VAL_RATIO, TEST_RATIO)"],"metadata":{"id":"e6mI0KH163-n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i, (image, label) in enumerate(train.take(16)):\n","  ax = plt.subplot(4, 4, i+1)\n","  plt.imshow(image)\n","  plt.title(info.features['label'].int2str(label))\n","  plt.axis('off')"],"metadata":{"id":"ENbCba_l65nT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def resize_rescale(image, label):\n","  image = tf.image.resize(image, (224, 224))/255\n","  return image, label"],"metadata":{"id":"5mDJc0M067jO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train = train.map(resize_rescale)\n","val = val.map(resize_rescale)\n","test = test.map(resize_rescale)"],"metadata":{"id":"4HzzktH57Ieq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train = train.shuffle(buffer_size=8, reshuffle_each_iteration=True).batch(32).prefetch(tf.data.AUTOTUNE)\n","val = val.shuffle(buffer_size=8, reshuffle_each_iteration=True).batch(32).prefetch(tf.data.AUTOTUNE)\n","test = test.batch(1)"],"metadata":{"id":"lQQCjJ2t7Ki6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["feature_extr_input = Input(shape=(224,224,3), name='Input Image')\n","x = Conv2D(filters=6, kernel_size =3, strides=1, padding='valid', activation='relu')(feature_extr_input)\n","x = BatchNormalization()(x)\n","x = MaxPool2D(pool_size=2, strides=2)(x)\n","x = Conv2D(filters=16, kernel_size =3, strides=1, padding='valid', activation='relu')(x)\n","x = BatchNormalization()(x)\n","output = MaxPool2D(pool_size=2, strides=2)(x)\n","feature_extractor = Model(inputs=feature_extr_input, outputs=output, name='Feature_Extractor')\n","feature_extractor.summary()"],"metadata":{"id":"WkNq9wrr7TOm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["func_input = Input(shape=(224,224,3), name='Input Image')\n","x = feature_extractor(func_input)\n","x = Flatten()(x)\n","x = Dense(100, activation='relu')(x)\n","x = BatchNormalization()(x)\n","x = Dense(10, activation='relu')(x)\n","x = BatchNormalization()(x)\n","func_output = Dense(1, activation='sigmoid')(x)\n","\n","LeNet_model = Model(inputs=func_input, outputs=func_output, name='LeNet')\n","LeNet_model.summary()"],"metadata":{"id":"5jpqMwE67V5z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Feature_extract_seq_model = tf.keras.Sequential([\n","                              InputLayer(input_shape = (224,224,3)),\n","                              Conv2D(filters=6, kernel_size =3, strides=1, padding='valid', activation='relu'),\n","                              BatchNormalization(),\n","                              MaxPool2D(pool_size=2, strides=2),\n","                              Conv2D(filters=16, kernel_size =3, strides=1, padding='valid', activation='relu'),\n","                              BatchNormalization(),\n","                              MaxPool2D(pool_size=2, strides=2)\n","\n","])\n","\n","Feature_extract_seq_model.summary()"],"metadata":{"id":"KS6AyXiG7qfT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = Feature_extract_seq_model(func_input)\n","x = Flatten()(x)\n","x = Dense(100, activation='relu')(x)\n","x = BatchNormalization()(x)\n","x = Dense(10, activation='relu')(x)\n","x = BatchNormalization()(x)\n","func_output = Dense(1, activation='sigmoid')(x)\n","\n","LeNet_model2 = Model(inputs=func_input, outputs=func_output, name='LeNet')\n","LeNet_model2.summary()"],"metadata":{"id":"FziGvkeZ7uWZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class FeatureExtractor(Layer):\n","  def __init__(self, filters, kernel_size, strides, padding, activation, pool_size,):\n","    super(FeatureExtractor, self).__init__()\n","\n","    self.conv_1 = Conv2D(filters = filters, kernel_size = kernel_size, strides = strides, padding = padding, activation = activation)\n","    self.batch_1 = BatchNormalization()\n","    self.pool_1 = MaxPool2D (pool_size = pool_size, strides= 2*strides)\n","\n","    self.conv_2 = Conv2D(filters = filters*2, kernel_size = kernel_size, strides = strides, padding = padding, activation = activation)\n","    self.batch_2 = BatchNormalization()\n","    self.pool_2 = MaxPool2D (pool_size = pool_size, strides= 2*strides)\n","\n","  def call(self, x, training):\n","\n","    x = self.conv_1(x)\n","    x = self.batch_1(x)\n","    x = self.pool_1(x)\n","\n","    x = self.conv_2(x)\n","    x = self.batch_2(x)\n","    x = self.pool_2(x)\n","\n","    return x\n","feature_sub_classed = FeatureExtractor(8, 3, 1, \"valid\", \"relu\", 2)"],"metadata":{"id":"Inau5_nn7zVH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["IM_SIZE = 224\n","func_input = Input(shape = (IM_SIZE, IM_SIZE, 3), name = \"Input Image\")\n","\n","x = feature_sub_classed(func_input)\n","\n","x = Flatten()(x)\n","\n","x = Dense(100, activation = \"relu\")(x)\n","x = BatchNormalization()(x)\n","\n","x = Dense(10, activation = \"relu\")(x)\n","x = BatchNormalization()(x)\n","\n","func_output = Dense(1, activation = \"sigmoid\")(x)\n","\n","lenet_model_func = Model(func_input, func_output, name = \"Lenet_Model\")\n","lenet_model_func.summary()"],"metadata":{"id":"UkNMSCFS73-M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class LenetModel(Model):\n","  def __init__(self):\n","    super(LenetModel, self).__init__()\n","\n","    self.feature_extractor = FeatureExtractor(8, 3, 1, \"valid\", \"relu\", 2)\n","\n","    self.flatten = Flatten()\n","\n","    self.dense_1 = Dense(100, activation = \"relu\")\n","    self.batch_1 = BatchNormalization()\n","\n","    self.dense_2 = Dense(10, activation = \"relu\")\n","    self.batch_2 = BatchNormalization()\n","\n","    self.dense_3 = Dense(1, activation = \"sigmoid\")\n","\n","  def call(self, x, training):\n","\n","    x = self.feature_extractor(x)\n","    x = self.flatten(x)\n","    x = self.dense_1(x)\n","    x = self.batch_1(x)\n","    x = self.dense_2(x)\n","    x = self.batch_2(x)\n","    x = self.dense_3(x)\n","\n","    return x\n","\n","lenet_sub_classed = LenetModel()\n","lenet_sub_classed(tf.zeros([1,224,224,3]))\n","lenet_sub_classed.summary()\n"],"metadata":{"id":"9MYbvWQi7-f9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CustomDense(Layer):\n","  def __init__(self, output_units, activation):\n","    super(CustomDense, self).__init__()\n","    self.output_units = output_units\n","    self.activation = activation\n","\n","  def build(self, input_features_shape):\n","    self.w = self.add_weight(shape = (input_features_shape[-1], self.output_units), initializer = \"random_normal\", trainable = True)\n","    self.b = self.add_weight(shape = (self.output_units,), initializer = \"random_normal\", trainable = True)\n","\n","  def call(self, input_features):\n","\n","    output = tf.matmul(input_features, self.w) + self.b\n","\n","    if(self.activation == \"relu\"):\n","      return tf.nn.relu(output)\n","\n","    elif(self.activation == \"sigmoid\"):\n","      return tf.math.sigmoid(output)\n","\n","    else:\n","      return output\n"],"metadata":{"id":"0bfdxLTe8EpI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["IM_SIZE = 224\n","LeNet_custom_model = tf.keras.Sequential([\n","                             InputLayer(input_shape = (IM_SIZE, IM_SIZE, 3)),\n","\n","                             Conv2D(filters = 6, kernel_size = 3, strides=1, padding='valid', activation = 'relu'),\n","                             BatchNormalization(),\n","                             MaxPool2D (pool_size = 2, strides= 2),\n","\n","                             Conv2D(filters = 16, kernel_size = 3, strides=1, padding='valid', activation = 'relu'),\n","                             BatchNormalization(),\n","                             MaxPool2D (pool_size = 2, strides= 2),\n","\n","                             Flatten(),\n","\n","                             CustomDense(100, activation = \"relu\"),\n","                             BatchNormalization(),\n","\n","                             CustomDense(10, activation = \"relu\"),\n","                             BatchNormalization(),\n","\n","                             CustomDense(1, activation = \"sigmoid\"),\n","\n","])\n","LeNet_custom_model.summary()"],"metadata":{"id":"xTlny9k68IZ0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class LossCallback(Callback):\n","  def on_epoch_end(self, epoch, logs):\n","    print(\"\\n For Epoch Number {} the model has a loss of {} \".format(epoch+1, logs[\"loss\"]))\n","\n","  def on_batch_end(self, batch, logs):\n","    print(\"\\n For Batch Number {} the model has a loss of {} \".format(batch+1, logs))"],"metadata":{"id":"hQvpVYzM-GJc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class LogImagesCallbackTensorBoard(Callback):\n","  def on_epoch_end(self, epoch, logs):\n","    labels = []\n","    inp = []\n","\n","    for x,y in test_dataset.as_numpy_iterator():\n","      labels.append(y)\n","      inp.append(x)\n","    labels = np.array([i[0] for i in labels])\n","    predicted = lenet_model.predict(np.array(inp)[:,0,...])\n","\n","    threshold = 0.5\n","\n","    cm = confusion_matrix(labels, predicted > threshold)\n","\n","    plt.figure(figsize=(8,8))\n","\n","    sns.heatmap(cm, annot=True,)\n","    plt.title('Confusion matrix - {}'.format(threshold))\n","    plt.ylabel('Actual')\n","    plt.xlabel('Predicted')\n","    plt.axis('off')\n","\n","    buffer = io.BytesIO()\n","    plt.savefig(buffer, format = 'png')\n","\n","    image = tf.image.decode_png(buffer.getvalue(), channels=3)\n","    image = tf.expand_dims(image, axis = 0)\n","\n","    CURRENT_TIME = datetime.datetime.now().strftime('%d%m%y - %h%m%s')\n","    IMAGE_DIR = './logs/' + CURRENT_TIME + '/images'\n","    image_writer = tf.summary.create_file_writer(IMAGE_DIR)\n","\n","    with image_writer.as_default():\n","      tf.summary.image(\"Training data\", image, step = epoch)"],"metadata":{"id":"sV-IAjGC-N6J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class LogImagesCallbackWandBPlot(Callback):\n","  def on_epoch_end(self, epoch, logs):\n","    labels = []\n","    inp = []\n","\n","    for x,y in test_dataset.as_numpy_iterator():\n","      labels.append(y)\n","      inp.append(x)\n","    labels = np.array([i[0] for i in labels])\n","    predicted = lenet_model.predict(np.array(inp)[:,0,...])\n","\n","    print(\"labels\", labels, labels.dtype)\n","    print(\"predicted\", predicted, predicted.dtype)\n","\n","    pred = []\n","\n","    for i in range(len(predicted)):\n","      if(predicted[i][0] < 0.5):\n","        pred.append([1,0])\n","      else:\n","        pred.append([0,1])\n","\n","    pred = np.array(pred)\n","\n","    # wandb.log({\"Confusion Matrix\" : wandb.plot.confusion_matrix(\n","    #     probs = pred,\n","    #     y_true=labels,\n","    #     class_names=[\"Parasitized\", \"Uninfected\"])})\n","\n","    wandb.log({\"ROC Curve\" : wandb.plot.roc_curve(\n","        y_true = labels,\n","        y_probas = pred,\n","        labels = ['Parasitized', 'Uninfected'],\n","    )})\n","\n","    wandb.log({'loss':logs['loss']})"],"metadata":{"id":"YUeiS_a2-T_v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","class LogImagesCallbackWandB(Callback):\n","  def on_epoch_end(self, epoch, logs):\n","    labels = []\n","    inp = []\n","\n","    for x,y in test_dataset.as_numpy_iterator():\n","      labels.append(y)\n","      inp.append(x)\n","    labels = np.array([i[0] for i in labels])\n","    predicted = lenet_model.predict(np.array(inp)[:,0,...])\n","\n","    threshold = 0.5\n","\n","    cm = confusion_matrix(labels, predicted > threshold)\n","\n","    plt.figure(figsize=(8,8))\n","\n","    sns.heatmap(cm, annot=True,)\n","    plt.title('Confusion matrix - {}'.format(threshold))\n","    plt.ylabel('Actual')\n","    plt.xlabel('Predicted')\n","    plt.axis('off')\n","\n","    buffer = io.BytesIO()\n","    plt.savefig(buffer, format = 'png')\n","\n","    image_array = tf.image.decode_png(buffer.getvalue(), channels=3)\n","\n","    images = wandb.Image(image_array, caption=\"Confusion Matrix for epoch: {}\".format(epoch))\n","\n","    wandb.log(\n","        {\"Confusion Matrix\": images})"],"metadata":{"id":"Mhjqjb3s-Y8J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["csv_callback = CSVLogger(\n","    'logs.csv', separator=',', append=True\n",")"],"metadata":{"id":"j4Ceqq_v-fFq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["es_callback = EarlyStopping(\n","    monitor='val_loss', min_delta=0, patience=2, verbose=1,\n","    mode='auto', baseline=None, restore_best_weights=False\n",")"],"metadata":{"id":"5g3VXw44-jpj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install -U tensorboard_plugin_profile\n","!rm -rf ./logs/\n","CURRENT_TIME = datetime.datetime.now().strftime('%d%m%y - %h%m%s')\n","METRIC_DIR = './logs/' + CURRENT_TIME + '/metrics'\n","train_writer = tf.summary.create_file_writer(METRIC_DIR)\n","LOG_DIR = './logs/'+ CURRENT_TIME\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=LOG_DIR, histogram_freq = 1, profile_batch = '100,132')"],"metadata":{"id":"vMbTawOIAhgb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def scheduler(epoch, lr):\n","  if epoch <= 1:\n","    learning_rate = lr\n","  else:\n","    learning_rate = lr * tf.math.exp(-0.1)\n","    learning_rate = learning_rate.numpy()\n","\n","  with train_writer.as_default():\n","    tf.summary.scalar('Learning Rate', data = learning_rate, step = epoch)\n","  return learning_rate\n","scheduler_callback = LearningRateScheduler(scheduler, verbose = 1)"],"metadata":{"id":"dqmxKbmtAvWB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["checkpoint_callback = ModelCheckpoint(\n","    'weights.{epoch:02d}-{val_loss:.2f}.hdf5', monitor='val_precision', verbose=0, save_best_only=True,\n","    save_weights_only=True, mode='auto', save_freq='epoch',\n",")"],"metadata":{"id":"gLFvNR0wAyRH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plateau_callback = ReduceLROnPlateau(\n","    monitor='val_accuracy', factor=0.1, patience=5, verbose=1\n",")"],"metadata":{"id":"c5y8XYWaA7Yg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["LeNet_custom_model.compile(loss=BinaryCrossentropy(), optimizer=Adam(learning_rate=0.01), metrics='accuracy')"],"metadata":{"id":"PyW62-_-9rU3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history = LeNet_custom_model.fit(train, epochs=20, validation_data=val, verbose=1)"],"metadata":{"id":"o1eFHAIx93nH"},"execution_count":null,"outputs":[]}]}