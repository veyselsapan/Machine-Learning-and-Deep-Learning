{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM0QZpUa7/KJYUeQeBIHl8Y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"t3vP63e9hnzY"},"outputs":[],"source":["import tensorflow as tf### models\n","import numpy as np### math computations\n","import matplotlib.pyplot as plt### plotting bar chart\n","import sklearn### machine learning library\n","import cv2## image processing\n","from sklearn.metrics import confusion_matrix, roc_curve### metrics\n","import seaborn as sns### visualizations\n","import datetime\n","import pathlib\n","import io\n","import os\n","import time\n","import random\n","from google.colab import files\n","from PIL import Image\n","import albumentations as A\n","import tensorflow_datasets as tfds\n","import tensorflow_probability as tfp\n","import matplotlib.cm as cm\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Layer\n","from tensorflow.keras.layers import (GlobalAveragePooling2D, Activation, MaxPooling2D, Add, Conv2D, MaxPool2D, Dense,\n","                                     Flatten, InputLayer, BatchNormalization, Input, Embedding, Permute,\n","                                     Dropout, RandomFlip, RandomRotation, LayerNormalization, MultiHeadAttention,\n","                                     RandomContrast, Rescaling, Resizing, Reshape)\n","from tensorflow.keras.losses import BinaryCrossentropy,CategoricalCrossentropy, SparseCategoricalCrossentropy\n","from tensorflow.keras.metrics import Accuracy,TopKCategoricalAccuracy, CategoricalAccuracy, SparseCategoricalAccuracy\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import (Callback, CSVLogger, EarlyStopping, LearningRateScheduler,\n","                                        ModelCheckpoint, ReduceLROnPlateau)\n","from tensorflow.keras.regularizers  import L2, L1\n","from tensorflow.train import BytesList, FloatList, Int64List\n","from tensorflow.train import Example, Features, Feature\n","from google.colab import drive"]},{"cell_type":"code","source":["CONFIGURATION = {\n","    \"BATCH_SIZE\": 32,\n","    \"IM_SIZE\": 256,\n","    \"LEARNING_RATE\": 1e-3,\n","    \"N_EPOCHS\": 20,\n","    \"DROPOUT_RATE\": 0.0,\n","    \"REGULARIZATION_RATE\": 0.0,\n","    \"N_FILTERS\": 6,\n","    \"KERNEL_SIZE\": 3,\n","    \"N_STRIDES\": 1,\n","    \"POOL_SIZE\": 2,\n","    \"N_DENSE_1\": 1024,\n","    \"N_DENSE_2\": 128,\n","    \"NUM_CLASSES\": 3,\n","    \"PATCH_SIZE\": 16,\n","    \"PROJ_DIM\": 768,\n","    \"CLASS_NAMES\": [\"angry\", \"happy\", \"sad\"],\n","}"],"metadata":{"id":"3ERzPfVYhyNl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_directory = \"/content/dataset/Emotions Dataset/Emotions Dataset/train\"\n","val_directory = \"/content/dataset/Emotions Dataset/Emotions Dataset/test\""],"metadata":{"id":"j8OgSqSWh2vK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install wandb\n","import wandb\n","from wandb.keras import WandbCallback\n","!wandb login\n","wandb.config = CONFIGURATION"],"metadata":{"id":"uhz-ylkCh422"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -q kaggle\n","! mkdir ~/.kaggle\n","! cp kaggle.json ~/.kaggle/\n","!chmod 600 /root/.kaggle/kaggle.json\n","!kaggle datasets download -d muhammadhananasghar/human-emotions-datasethes"],"metadata":{"id":"tHKwoHBnh7Ir"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!unzip \"/content/human-emotions-datasethes.zip\" -d \"/content/dataset/\""],"metadata":{"id":"QqvZezrHh9HZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = tf.keras.utils.image_dataset_from_directory(\n","    train_directory,\n","    labels='inferred',\n","    label_mode='categorical',\n","    class_names=CONFIGURATION[\"CLASS_NAMES\"],\n","    color_mode='rgb',\n","    batch_size=CONFIGURATION[\"BATCH_SIZE\"],\n","    image_size=(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"]),\n","    shuffle=True,\n","    seed=99,\n",")\n","val_dataset = tf.keras.utils.image_dataset_from_directory(\n","    val_directory,\n","    labels='inferred',\n","    label_mode='categorical',\n","    class_names=CONFIGURATION[\"CLASS_NAMES\"],\n","    color_mode='rgb',\n","    batch_size=1,#CONFIGURATION[\"BATCH_SIZE\"],\n","    image_size=(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"]),\n","    shuffle=True,\n","    seed=99,\n",")"],"metadata":{"id":"Vm-3sR6mh9uy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in val_dataset.take(1):\n","  print(i)"],"metadata":{"id":"r8cRB1TEiAB7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize = (12,12))\n","\n","for images, labels in train_dataset.take(1):\n","  for i in range(16):\n","    ax = plt.subplot(4,4, i+1)\n","    plt.imshow(images[i]/255.)\n","    plt.title(CONFIGURATION[\"CLASS_NAMES\"][tf.argmax(labels[i], axis = 0).numpy()])\n","    plt.axis(\"off\")"],"metadata":{"id":"t0bDs4OmiDVU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### tf.keras.layer augment\n","augment_layers = tf.keras.Sequential([\n","  RandomRotation(factor = (-0.025, 0.025)),\n","  RandomFlip(mode='horizontal',),\n","  RandomContrast(factor=0.1),\n","])\n","def augment_layer(image, label):\n","  return augment_layers(image, training = True), label"],"metadata":{"id":"rLvut7XjiFp3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_dataset = (\n","    train_dataset\n","    .map(augment_layer, num_parallel_calls = tf.data.AUTOTUNE)\n","    .prefetch(tf.data.AUTOTUNE)\n",")"],"metadata":{"id":"tDmyGJRgiIpd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["validation_dataset = (\n","    val_dataset\n","    .prefetch(tf.data.AUTOTUNE)\n",")"],"metadata":{"id":"iQgCxF_CiNKO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["resize_rescale_layers = tf.keras.Sequential([\n","       Resizing(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"]),\n","       Rescaling(1./255),\n","])"],"metadata":{"id":"CnRWdS6IiPh5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_dataset"],"metadata":{"id":"6Ddl1GIgiSQ3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["validation_dataset"],"metadata":{"id":"CP6Xv6M6iUay"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vgg_backbone = tf.keras.applications.vgg16.VGG16(\n","    include_top=False,\n","    weights= None,\n","    input_shape=(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"], 3),\n","\n",")"],"metadata":{"id":"u48d9vl-iXJS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def is_conv(layer_name):\n","  if 'conv' in layer_name:\n","    return True\n","  else:\n","    return False"],"metadata":{"id":"xpnZxWKxoWxh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["feature_maps = [layer.output for layer in vgg_backbone.layers[1:] if is_conv(layer.name)]\n","feature_map_model = Model(\n","    inputs = vgg_backbone.input,\n","    outputs = feature_maps\n",")\n","\n","feature_map_model.summary()"],"metadata":{"id":"TUCucEwEoXRn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_image = cv2.imread(\"/content/dataset/Emotions Dataset/Emotions Dataset/test/happy/111073.jpg\")\n","test_image = cv2.resize(test_image, (CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"]))\n","\n","im = tf.constant(test_image, dtype = tf.float32)\n","im = tf.expand_dims(im, axis = 0)\n","\n","f_maps = feature_map_model.predict(im)"],"metadata":{"id":"7MVJh1wkobqp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(f_maps))"],"metadata":{"id":"urpE8AVBoegn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(len(f_maps)):\n","  print(f_maps[i].shape)"],"metadata":{"id":"76ArYXoHogni"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(len(f_maps)):\n","  plt.figure(figsize = (256,256))\n","  f_size = f_maps[i].shape[1]\n","  n_channels = f_maps[i].shape[3]\n","  joint_maps = np.ones((f_size, f_size*n_channels ))\n","\n","  axs = plt.subplot(len(f_maps), 1, i+1)\n","\n","  for j in range(n_channels):\n","    joint_maps[:, f_size*j:f_size*(j+1)] = f_maps[i][..., j]\n","\n","  plt.imshow(joint_maps[:,0:512])\n","  plt.axis(\"off\")\n"],"metadata":{"id":"x7OP2DFhojGG"},"execution_count":null,"outputs":[]}]}